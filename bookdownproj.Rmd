--- 
title: "ESS 580 Bookdown"
author: "Madeline Carlson"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
---

# Introduction

This is a book of all the assignments done in Introduction to Environmental 
Data Science (ESS 580A-7).

```{r setup, warning=F, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidyr)
library(dataRetrieval)
library(dygraphs)
library(xts)
library(revealjs)
library(ggthemes)
library(lubridate)
library(gridExtra)
library(rvest)
library(readxl)
library(sf)
library(mapview)
library(LAGOSNE)
library(USAboundaries)
library(R.matlab)
library(rnassqs)
library(maps)
library(mapdata)

setwd("/Users/mjcarlson/Desktop/ESS580 Environmental Data Science/ESS580Bookdown")
```



<!--chapter:end:index.Rmd-->

# Assignment 1: R Markdown Examples

## Methods

The Poudre River at Lincoln Bridge is:

  - Downstream of only a little bit of urban stormwater

  - Near Odell Brewing CO
  
  - Near an open space area and the Poudre River Trail
  
  - **Downstream of many agricultural diversions**


### SiteDescription

![](https://waterdata.usgs.gov/nwisweb/local/state/co/text/pics/06752260big.jpg)


### Data Acquisition and Plotting tests

### Data Download


```{r downloader}

q <- readNWISdv(siteNumbers = '06752260',
                parameterCd = '00060',
                startDate = '2017-01-01',
                endDate = '2022-01-01') %>%
  rename(q = 'X_00060_00003')


```



### Static Data Plotter


```{r, warning = FALSE, fig.width = 8, fig.height = 5}

ggplot(q, aes(x = Date, y = q)) + 
  geom_line() + 
  ylab('Q (cfs)') + 
  ggtitle('Discharge in the Poudre River, Fort Collins')

```


### Interactive Data Plotter


```{r}

q_xts <- xts(q$q, order.by = q$Date)


dygraph(q_xts) %>%
  dyAxis("y", label = "Discharge (cfs)") 
```



## Assignment. 


This assignment will be primarily about demonstrating some expertice in using
RMarkdown, since we will be using Rmds as the primary form of homework and 
assignments. With that in mind, your assignment for this homework is to:


1) Fork the example repository into your personal GitHub

2) Create an RStudio project from your Personal clone of the Repo. 

3) Create a table of contents that is floating, but displays three levels of
headers instead of two (by editing the content at the beginning of the document)

4) Make a version of the `dygraph` with points and lines by using rstudio's
dygraph [guide](https://rstudio.github.io/dygraphs/)

5) Writing a paragraph on the Poudre river with at least three hyperlinks,
**two bolded sections**, and one *italicized phrase*. The content of this paragraph
is not vital, but try to at least make it true and interesting, and, of course,
don't plagiarize. 

6) Knit that document, and then git commit and push to your personal GitHub.

7) Use the GitHub -> Settings -> Pages tab to create a website of your report.

8) Bonus, make the timestamp in the header dynamic. As in it only adds
todays date, not just a static date you enter. 

9) Bonus, create an "index_talk.Rmd" version of your document using the
`revealjs` package. Add link to your original report-style document. 

 


## DyGraph example. 

```{r}

dygraph(q_xts) %>%
  dyAxis("y", label = "Discharge (cfs)")%>%
  dyOptions(drawPoint=T, pointSize = 2)

```


## Poudre Paragraph

The **Cache la Poudre River** is a Colorado river that has headwaters in Rocky Mountain National park and flows 126 miles then into the **South Platte River**. It is great for tourists and [recreation](https://www.visitftcollins.com/things-to-do/parks-open-spaces/cache-la-poudre-river/) due to its proximity to Fort Collins. It also is home to one of the 55 [National Heritage Areas](https://www.nps.gov/places/cache-la-poudre-river-national-heritage-area.htm) in the US. The Poudre is known for being Colorado's only *Wild and Scenic Rivers* as well as having high quality drinking [water](https://www.poudrewatershed.org/cache-la-poudre-watershed#:~:text=High%20quality%20drinking%20water&text=The%20City%20of%20Fort%20Collins,watershed%20is%20high%20severity%20wildfire.).



<!--chapter:end:02-rmarkdown_examples.Rmd-->

# Assignment 2: Fire Data Wrangle
```{r, include=F}
library(tidyverse)
library(tidyr)
library(ggthemes)
library(lubridate)
library(gridExtra)

# Now that we have learned how to munge (manipulate) data
# and plot it, we will work on using these skills in new ways

knitr::opts_knit$set(root.dir='..')
```


```{r dataread, warning=F,message=F}
####-----Reading in Data and Stacking it ----- ####
#Reading in files
files <- list.files('/Users/mjcarlson/Desktop/ESS580 Environmental Data Science/ESS580Bookdown/data_hayman',full.names=T)


#Read in individual data files
ndmi <- read_csv(files[1]) %>% 
  rename(burned=2,unburned=3) %>%
  mutate(data='ndmi')


ndsi <- read_csv(files[2]) %>% 
  rename(burned=2,unburned=3) %>%
  mutate(data='ndsi')

ndvi <- read_csv(files[3])%>% 
  rename(burned=2,unburned=3) %>%
  mutate(data='ndvi')

# Stack as a tidy dataset
full_long <- rbind(ndvi,ndmi,ndsi) %>%
  gather(key='site',value='value',-DateTime,-data) %>%
  filter(!is.na(value))


```




## Question 1) 

What is the correlation between NDVI and NDMI? - here I want you to
convert the full_long dataset in to a wide dataset using the 
function "spread" and then make a plot that shows the correlation s a
function of if the site was burned or not (x axis should be ndmi)
You should exclude winter months and focus on summer months


```{r}
full_wide=full_long%>%
  spread(data, value)%>%
  filter_if(is.numeric,all_vars(!is.na(.)))

full_wide_xwinter<-full_wide%>%
  mutate(month=month(DateTime))%>%
  filter(month %in% c(5, 6, 7, 8, 9))
  

ggplot(full_wide_xwinter, aes(x=ndmi, y=ndvi, color=site))+
  geom_point()

```

There is a strong positive correlation between NDVI and NDMI for both burned and unburned sites. The burned sights generally have a lower NDVI which is to be expected due to the reduced amount of vegetation as a result of the burn.


## Question 2 

2) What is the correlation between average NDSI (normalized
 snow index) for January - April and average NDVI for June-August?
In other words, does the previous year's snow cover influence vegetation
 growth for the following summer?

```{r, warning=F, message=F}
#ndsi winter data frame
ndsi_annual<-full_wide%>%
  mutate(month=month(DateTime))%>%
  mutate(year=year(DateTime))%>%
  filter(month %in% c(1,2,3,4))%>%
  subset(., select= -c(ndvi,ndmi))%>%
  group_by(site, year)%>%
  summarize(mean_ndsi=mean(ndsi))

#ndvi summer data frame
ndvi_annual<-full_wide%>%
  mutate(month=month(DateTime))%>%
  mutate(year=year(DateTime))%>%
  filter(month %in% c(6,7,8))%>%
  subset(., select = -c(ndsi,ndmi))%>%
  group_by(site, year)%>%
  summarize(mean_ndvi=mean(ndvi))

#combine data frames
combined<-inner_join(ndvi_annual,ndsi_annual)

#correlation
cor(combined$mean_ndsi, combined$mean_ndvi)

#correlation plot
ggplot(combined, aes(x=mean_ndsi, y=mean_ndvi, color=year, shape=site))+
  geom_point(size=2)

```

There is minimal correlation between the previous years snowfall and vegetation growth. The correlation coefficient is 0.18, meaning that the positive correlation is close to 0 or nearly no linear relationship.


## Q3

How is the snow effect from question 2 different between pre- and post-burn
and burned and unburned? 

```{r, warning=F, message=F}
#pre and post burn
preburn<-c(1984:2001)
postburn<-c(2003:2019)

#preburn
#preburn annual ndsi
preburn_annualndsi<-full_wide[c(1,2,4)] %>%
  mutate(month=month(DateTime))%>%
  mutate(year=year(DateTime))%>%
  filter(month %in% c(1,2,3,4))%>% 
  filter(year %in% preburn)%>%
  group_by(site, year)%>%
  summarize(mean_preburnndsi=mean(ndsi))

#preburn annual ndvi
preburn_annualndvi<-full_wide[c(1,2,5)] %>%
  mutate(month=month(DateTime))%>%
  mutate(year=year(DateTime))%>%
  filter(month %in% c(6,7,8))%>% 
  filter(year %in% preburn)%>%
  group_by(site, year)%>%
  summarize(mean_preburnndvi=mean(ndvi))

#combine pre burn
combinepre<-inner_join(preburn_annualndsi,preburn_annualndvi)

#correlation pre burn
cor_preburn<-cor(combinepre$mean_preburnndsi, combinepre$mean_preburnndvi)

#plot combine pre burn
p1<-ggplot(combinepre, aes(x=mean_preburnndsi, y=mean_preburnndvi, color=site))+
  geom_point(size=2)+
  labs(title="Preburn",
       x="Mean preburn NDSI",
       y= "Mean preburn NDVI")

#postburn
#postburn annual ndsi
postburn_annualndsi<-full_wide[c(1,2,4)] %>%
  mutate(month=month(DateTime))%>%
  mutate(year=year(DateTime))%>%
  filter(month %in% c(1,2,3,4))%>% 
  filter(year %in% postburn)%>%
  group_by(site, year)%>%
  summarize(mean_postburnndsi=mean(ndsi))

#postburn annual ndvi
postburn_annualndvi<-full_wide[c(1,2,5)] %>%
  mutate(month=month(DateTime))%>%
  mutate(year=year(DateTime))%>%
  filter(month %in% c(6,7,8))%>% 
  filter(year %in% postburn)%>%
  group_by(site, year)%>%
  summarize(mean_postburnndvi=mean(ndvi))

#combine post burn
combinepost<-inner_join(postburn_annualndsi,postburn_annualndvi)

#correlation post burn
cor_postburn<-cor(combinepost$mean_postburnndsi, combinepost$mean_postburnndvi)

#plot combine post burn
p2<-ggplot(combinepost, aes(x=mean_postburnndsi, y=mean_postburnndvi, color=site))+
  geom_point(size=2)+
   labs(title="Postburn",
       x="Mean postburn NDSI",
       y= "Mean postburn NDVI")

#burned and unburned correlation
#burned
#burned annual ndsi
burned_annualndsi<-full_wide[c(1,2,4)] %>%
  mutate(month=month(DateTime))%>%
  mutate(year=year(DateTime))%>%
  filter(month %in% c(1,2,3,4))%>% 
  filter(site %in% "burned")%>%
  group_by(site, year)%>%
  summarize(mean_burnndsi=mean(ndsi))

#burned annual ndvi
burned_annualndvi<-full_wide[c(1,2,5)] %>%
  mutate(month=month(DateTime))%>%
  mutate(year=year(DateTime))%>%
  filter(month %in% c(6,7,8))%>% 
  filter(site %in% "burned")%>%
  group_by(site, year)%>%
  summarize(mean_burnndvi=mean(ndvi))

#combine burned
combineburned<-inner_join(burned_annualndsi,burned_annualndvi)

#correlation burned
cor_burned<-cor(combineburned$mean_burnndsi, combineburned$mean_burnndvi)

#plot combine burn
p3<-ggplot(combineburned, aes(x=mean_burnndsi, y=mean_burnndvi))+
  geom_point(size=2)+
   labs(title="Burned",
       x="Mean burned NDSI",
       y= "Mean burned NDVI")

#unburned
#unburned annual ndsi
unburned_annualndsi<-full_wide[c(1,2,4)] %>%
  mutate(month=month(DateTime))%>%
  mutate(year=year(DateTime))%>%
  filter(month %in% c(1,2,3,4))%>% 
  filter(site %in% "unburned")%>%
  group_by(site, year)%>%
  summarize(mean_unburnndsi=mean(ndsi))

#unburned annual ndvbi
unburned_annualndvi<-full_wide[c(1,2,5)] %>%
  mutate(month=month(DateTime))%>%
  mutate(year=year(DateTime))%>%
  filter(month %in% c(6,7,8))%>% 
  filter(site %in% "unburned")%>%
  group_by(site, year)%>%
  summarize(mean_unburnndvi=mean(ndvi))

#combine unburned
combineunburned<-inner_join(unburned_annualndsi,unburned_annualndvi)

#correlation unburned
cor_unburned<-cor(combineunburned$mean_unburnndsi, combineunburned$mean_unburnndvi)

#plot combine unburn
p4<-ggplot(combineunburned, aes(x=mean_unburnndsi, y=mean_unburnndvi))+
  geom_point(size=2)+
   labs(title="Unburned",
       x="Mean unbured NDSI",
       y= "Mean unburned NDVI")

grid.arrange(p1, p2, p3, p4)

cor_preburn
cor_postburn
cor_burned
cor_unburned
```

There is no correlation between snow and vegetation for preburn, burned and unburned sites. There is a slight positive linear correlation between snow and vegetation for postburn sites with a value of 0.24.


## Question 4

What month is the greenest month on average? 

```{r, warning=F, message=F}
ndvi_month<-full_wide[c(1,2,5)]%>%
  mutate(month=month(DateTime))%>%
  mutate(year=year(DateTime))%>%
  group_by(month)%>%
  summarize(mean_ndvi=mean(ndvi))

ggplot(ndvi_month, aes(x=month, y=mean_ndvi, color=month))+
  geom_point(size=3)+
  geom_line()

ndvi_month2<-full_wide[c(1,2,5)]%>%
  mutate(month=month(DateTime))%>%
  mutate(year=year(DateTime))%>%
  group_by(site,month)%>%
  summarize(mean_ndvi=mean(ndvi))

ggplot(ndvi_month2, aes(x=month, y=mean_ndvi, color=site))+
  geom_point()

```

The greenest month on average is August. The greenest month for burned sites is also August but the greenest month for unburned sights is September.


## Question 5) 

What month is the snowiest on average?

```{r, warning=F, message=FALSE}
ndsi_month<-full_wide[c(1,2,4)]%>%
  mutate(month=month(DateTime))%>%
  mutate(year=year(DateTime))%>%
  group_by(month)%>%
  summarize(mean_ndsi=mean(ndsi))

ggplot(ndsi_month, aes(x=month, y=mean_ndsi, color=month))+
  geom_point(size=3)+
  geom_line()

ndsi_month2<-full_wide[c(1,2,4)]%>%
  mutate(month=month(DateTime))%>%
  mutate(year=year(DateTime))%>%
  group_by(site, month)%>%
  summarize(mean_ndsi=mean(ndsi))

ggplot(ndsi_month2, aes(x=month, y=mean_ndsi, color=site))+
  geom_point()

```

The snowiest month on average is January. There is minimal difference in the NDSI between the burned and unburned sites, however, the unburned does have February as the snowiest month.


<!--chapter:end:03-fire_data_wrangle.Rmd-->

# Assignment 3: Snow Function Iteration

```{r, include=F}
library(rvest)
library(tidyverse)
library(lubridate)
library(readxl)
```


## Simple web scraping

R can read html using either rvest, xml, or xml2 packages. Here we are going to navigate to the Center for Snow and Avalance Studies  [Website](https://snowstudies.org/archived-data/) and read a table in. This table contains links to data we want to programatically download for three sites. We don't know much about these sites, but they contain incredibly rich snow, temperature, and precip data. 


## Assignment:

1. Extract the meteorological data URLs. Here we want you to use the `rvest` package to get the URLs for the `SASP forcing` and `SBSP_forcing` meteorological datasets.

```{r}
site_url <- 'https://snowstudies.org/archived-data/'

#Read the web url
webpage <- read_html(site_url)

links <- webpage %>%
  html_nodes('a') %>%
  .[grepl('forcing',.)] %>%
  html_attr('href')
links
```


2. Download the meteorological data. Use the `download_file` and `str_split_fixed` commands to download the data and save it in your data folder. You can use a for loop or a map function. 

```{r}
#Grab only the name of the file by splitting out on forward slashes
splits <- str_split_fixed(links,'/',8)

#Keep only the 8th column
dataset <- splits[,8] 

#generate a file list for where the data goes
file_names <- paste0('data_snow/',dataset)

#for(i in 1:3){
#  download.file(links[i],destfile=file_names[i])
#}

#downloaded <- file.exists(file_names)

#evaluate <- !all(downloaded)

```


3. Write a custom function to read in the data and append a site column to the data. 

```{r}

# this code grabs the variable names from the metadata pdf file
library(pdftools)
headers <- pdf_text('https://snowstudies.org/wp-content/uploads/2022/02/Serially-Complete-Metadata-text08.pdf') %>%
  readr::read_lines(.) %>%
  trimws(.) %>%
  str_split_fixed(.,'\\.',2) %>%
  .[,2] %>%
  .[1:26] %>%
  str_trim(side = "left")

```


```{r}
meteorological_reader<-function(file_names){
  name=str_split_fixed(file_names,'/',2)[,2]
  name2=str_split_fixed(file_names,'/',4)[,2]
  df<-read.delim(file_names, header = F, sep = "", col.names = headers, skip = 4)%>%
    select(1:14)%>%
    mutate(site=name2)
}

```


4. Use the `map` function to read in both meteorological files. Display a summary of your tibble.
 
```{r}

fulldata<-map_dfr(file_names, meteorological_reader)

unique(fulldata$site)

```



5. Make a line plot of mean temp by year by site (using the `air temp [K]` variable). Is there anything suspicious in the plot? Adjust your filtering if needed.

```{r}
annual_mean<-fulldata%>%
  group_by(site, year)%>%
  summarize(mean_air_temp=mean(air.temp..K.))

ggplot(annual_mean, aes(x=year, y=mean_air_temp, color=site)) + 
  geom_line(size=2)+
  labs(title="Annual mean air temperature",
       x="Year",
       y= "Temperature (K)")+
  theme(legend.position = c(.7, .25))
  
```

2003 is much colder than the other years which is suspicious. When looking at the data we can see that the data starts in November so it is not a complete year of data and only includes two colder months.

```{r}
annual_mean<-fulldata%>%
  group_by(site, year)%>%
  filter(year>2003)%>%
  summarize(mean_air_temp=mean(air.temp..K.))

ggplot(annual_mean, aes(x=year, y=mean_air_temp, color=site)) + 
  geom_line(size=2)+
  labs(title="Annual mean air temperature",
       x="Year",
       y= "Temperature (K)")+
  theme(legend.position = c(.7, .25))
```


6. Write a function that makes line plots of monthly average temperature at each site for a given year. Use a for loop to make these plots for 2005 to 2010. Are monthly average temperatures at the Senator Beck Study Plot ever warmer than the Swamp Angel Study Plot?
Hint: https://ggplot2.tidyverse.org/reference/print.ggplot.html

```{r, warning=FALSE,message=FALSE}

#monthly_mean<-fulldata%>%
#  group_by(month, year, site)%>%
#  summarize(monthly_air_temp = mean(air.temp..K.))

#ggplot(monthly_mean, aes(x = month, y = monthly_air_temp, color = site)) +
#  facet_wrap(~year) +
#  geom_line()

monthplot<-function(fulldata, year){
  monthlytemp<-fulldata%>%
    group_by(month, year, site)%>%
    summarize(monthly_air_temp = mean(air.temp..K.))%>%
    filter(yr == year)
  
  plots<-ggplot(monthlytemp, aes(x = month, y = monthly_air_temp, color = site))+
    geom_line(size=2)+
    labs(title = monthlytemp$year,
         x="Month",
         y="Temperature (K)")
  
  print(plots)
}

years<-c(2005, 2006, 2007, 2008, 2009, 2010)

for (yr in years){
  monthplot(fulldata, year)
}
```


<!--chapter:end:04-snow_function_iteration.Rmd-->

# Assignment 4a: LAGOS Spacial Analysis

```{r, include=F}
library(tidyverse) # Tidy packages
library(sf) #Spatial package that can read and create shapefiles 
library(mapview) #Interactive maps
library(LAGOSNE) #Lots and lots of clean lake data
library(USAboundaries) #USA states and counties
```


## LAGOS Analysis


### Loading in data, first download and then specifically grab the locus (or site lat longs)

```{r data-read}
# #Lagos download script
#LAGOSNE::lagosne_get(dest_folder = LAGOSNE:::lagos_path())


#Load in lagos
lagos <- lagosne_load()

#Grab the lake centroid info
lake_centers <- lagos$locus


```



### Convert to spatial data
```{r}
#Look at the column names
#names(lake_centers)

#Look at the structure
#str(lake_centers)

#View the full dataset
#View(lake_centers %>% slice(1:100))

spatial_lakes <- st_as_sf(lake_centers,coords=c('nhd_long','nhd_lat'),
                          crs=4326) %>%
  st_transform(2163)

#Subset for plotting
subset_spatial <- spatial_lakes %>%
  slice(1:100) 

subset_baser <- spatial_lakes[1:100,]

#Dynamic mapviewer
mapview(subset_spatial)

```


### Subset to only Minnesota

```{r}
states <- us_states()

#Plot all the states to check if they loaded
#mapview(states)
minnesota <- states %>%
  filter(name == 'Minnesota') %>%
  st_transform(2163)

#Subset lakes based on spatial position
minnesota_lakes <- spatial_lakes[minnesota,]

#Plotting the first 1000 lakes
minnesota_lakes %>%
  arrange(-lake_area_ha) %>%
    slice(1:1000) %>%
  mapview(.,zcol = 'lake_area_ha')
```



## In-Class work


### 1) Show a map outline of Iowa and Illinois (similar to Minnesota map upstream)

```{r}
#mapview(states)
Icombined <- states %>%
  filter(name %in% c('Illinois', 'Iowa')) %>%
  st_transform(2163)

I_lakes <- spatial_lakes[Icombined,]

#Iowa
Iowa <- states %>%
  filter(name == 'Iowa') %>%
  st_transform(2163)

#Illinois
Illinois <- states %>%
  filter(name == 'Illinois') %>%
  st_transform(2163)

#rbind states
IA_IL<-rbind(Iowa,Illinois)
mapview(IA_IL)

```



### 2) Subset LAGOS data to these sites, how many sites are in Illinois and Iowa  combined? How does this compare to Minnesota?

```{r}
illinois_lakes <- spatial_lakes[Illinois,]
iowa_lakes <- spatial_lakes[Iowa,]

#rbind lake data
IA_IL2<-rbind(iowa_lakes, illinois_lakes)

#count
str(IA_IL2$lagoslakeid)
str(minnesota_lakes$lagoslakeid)

```

There are about twice as many lakes in Minnesota, 29038, as there are in Iowa and Illinois combined, 16466.


### 3) What is the distribution of lake size in Iowa vs. Minnesota?

- Here I want to see a histogram plot with lake size on x-axis and frequency on 
y axis (check out geom_histogram)

```{r}
ggplot(iowa_lakes, aes(x=lake_area_ha))+
  geom_histogram()+
  scale_x_log10()+
  labs(title = "Iowa",
    x="log(lake area(ha))")

ggplot(minnesota_lakes, aes(x=lake_area_ha))+
  geom_histogram()+
  scale_x_log10()+
  labs(title = "Minnesota",
    x="log(lake area (ha))")

```

There are more larger lakes in Minnesota than in Iowa where the lakes are much smaller.


### 4) Make an interactive plot of lakes in Iowa and Illinois and color them  by lake area in hectares

```{r}
I_lakes %>%
  arrange(-lake_area_ha) %>%
    slice(1:1000) %>%
  mapview(.,zcol = 'lake_area_ha')

```


### 5) What other data sources might we use to understand how reservoirs and  natural lakes vary in size in these three states? 

Using information such as the perimeter or the lake and lake depth would give another source of data to compare the sizes of the lakes.

<!--chapter:end:05-LAGOS_spacial_analysis.Rmd-->

# Assignment 4b: LAGOS Spacial Analysis

```{r, include=F}
library(tidyverse) # Tidy packages
library(sf) #Spatial package that can read and create shapefiles 
library(mapview) #Interactive maps
library(LAGOSNE) #Lots and lots of clean lake data
library(USAboundaries) #USA states and counties
library(lubridate) #For dealing with date and time
```


### Loading in data, first download and then specifically grab the locus (or site lat longs)
```{r}
#Lagos download script
#lagosne_get(dest_folder = LAGOSNE:::lagos_path(),overwrite=T)

#Load in lagos
lagos <- lagosne_load()


#Grab the lake centroid info
lake_centers <- lagos$locus

# Make an sf object 
spatial_lakes <- st_as_sf(lake_centers,coords=c('nhd_long','nhd_lat'),
                          crs=4326)

#Grab the water quality data
nutr <- lagos$epi_nutr

#Look at column names
#names(nutr)
```

### Subset columns nutr to only keep key info that we want


```{r}
clarity_only <- nutr %>%
  select(lagoslakeid,sampledate,chla,doc,secchi) %>%
  mutate(sampledate = as.character(sampledate) %>% ymd(.))

```


### Keep sites with at least 200 observations 

```{r}

#Look at the number of rows of dataset
#nrow(clarity_only)

chla_secchi <- clarity_only %>%
  filter(!is.na(chla),
         !is.na(secchi))

# How many observatiosn did we lose?
# nrow(clarity_only) - nrow(chla_secchi)


# Keep only the lakes with at least 200 observations of secchi and chla
chla_secchi_200 <- chla_secchi %>%
  group_by(lagoslakeid) %>%
  mutate(count = n()) %>%
  filter(count > 200)


```


### Join water quality data to spatial data

```{r}
spatial_200 <- inner_join(spatial_lakes,chla_secchi_200 %>%
                            distinct(lagoslakeid,.keep_all=T),
                          by='lagoslakeid')


```

### Mean Chl_a map

```{r}
### Take the mean chl_a and secchi by lake

mean_values_200 <- chla_secchi_200 %>%
  # Take summary by lake id
  group_by(lagoslakeid) %>%
  # take mean chl_a per lake id
  summarize(mean_chl = mean(chla,na.rm=T),
            mean_secchi=mean(secchi,na.rm=T)) %>%
  #Get rid of NAs
  filter(!is.na(mean_chl),
         !is.na(mean_secchi)) %>%
  # Take the log base 10 of the mean_chl
  mutate(log10_mean_chl = log10(mean_chl))%>%
  #Take the log base 10 of the mean_secchi
  mutate(log10_mean_secchi = log10(mean_secchi))

#Join datasets
mean_spatial <- inner_join(spatial_lakes,mean_values_200,
                          by='lagoslakeid') 

#Make a map
mapview(mean_spatial,zcol='log10_mean_chl')
```


## Class work

### 1) What is the correlation between Secchi Disk Depth and Chlorophyll a for  sites with at least 200 observations?

- Here, I just want a plot of chla vs secchi for all sites 

```{r, warning=F}
#Your code here
ggplot(chla_secchi_200, aes(y = chla, x = secchi))+
  geom_point(size = 1, shape = 1)+
  scale_x_log10()+
  scale_y_log10()
```


### Why might this be the case? 

When there is more chlorophyll in the water it is more turbid and therefore harder to see the Secchi disk.

### 2) What states have the most data? 

### 2a) First you will need to make a lagos spatial dataset that has the total  number of counts per site.

```{r, warning=FALSE}
## Your code here
site_counts<-nutr%>%
  group_by(lagoslakeid)%>%
  summarize(count = n())

spatial_counts<-inner_join(spatial_lakes, site_counts%>%
                             distinct(lagoslakeid,.keep_all = T),
                           by = "lagoslakeid")

```


### 2b) Second, you will need to join this point dataset to the us_boundaries  data. 

```{r, warning=F}
## Your code here
states_lagos<-lagos$state

states_lakes <- inner_join(spatial_counts, states_lagos, by="state_zoneid")
```


### 2c) Then you will want to group by state and sum all the observations in  that state and arrange that data from most to least toatl observations per state. 

```{r}
## Your code here. 
sum_states<-states_lakes%>%
  as.data.frame()%>%
  select(-geometry)%>%
  group_by(state_name)%>%
  summarize(sum_counts = sum(count))%>%
  arrange(desc(sum_counts))

head(sum_states)
```

Minnesota has the most data followed by Wisconsin and Michigan

### 3 Is there a spatial pattern in Secchi disk depth for lakes with at least 200  observations?

```{r}
## Your code here
mean_spatial%>%
  arrange(-mean_secchi) %>%
  mapview(.,zcol = 'mean_secchi')

mapview(mean_spatial,zcol='log10_mean_secchi')

```

There is not a spatial pattern in Secchi disk depths, there are some clusters of locations where there are many lakes with more than 200 observations but this has more to do with lake density than Secchi disk depths.


<!--chapter:end:06-LAGOS_spacial_analysis.Rmd-->

# Assignment 5: Weather Corn Regression

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(R.matlab)
library(rnassqs)
library(maps)
library(mapdata)
```

## Weather Data Analysis

### Load the PRISM daily maximum temperatures

```{r tmax data}

# daily max temperature
# dimensions: counties x days x years
prism <- readMat("data_corn/prismiowa.mat")

# look at county #1
t_1981_c1 <- prism$tmaxdaily.iowa[1,,1]
t_1981_c1[366]
plot(1:366, t_1981_c1, type = "l")

ggplot() +
  geom_line(mapping = aes(x=1:366, y = t_1981_c1)) +
  theme_bw() +
  xlab("day of year") +
  ylab("daily maximum temperature (°C)") +
  ggtitle("Daily Maximum Temperature, Iowa County #1")


```


```{r tidying up}

# assign dimension names to tmax matrix
dimnames(prism$tmaxdaily.iowa) <- list(prism$COUNTYFP, 1:366, prism$years)

# converted 3d matrix into a data frame
tmaxdf <- as.data.frame.table(prism$tmaxdaily.iowa)

# relabel the columns
colnames(tmaxdf) <- c("countyfp","doy","year","tmax")
tmaxdf <- tibble(tmaxdf)

```

## Temperature trends

### Summer temperature trends: Winneshiek County

```{r temp trends, message=FALSE}

tmaxdf$doy <- as.numeric(tmaxdf$doy)
tmaxdf$year <- as.numeric(as.character(tmaxdf$year))

winnesummer <- tmaxdf %>%
  filter(countyfp==191 & doy >= 152 & doy <= 243) %>%
  group_by(year) %>%
  summarize(meantmax = mean(tmax))

ggplot(winnesummer, mapping = aes(x = year, y = meantmax)) +
  geom_point() +
  theme_bw() +
  labs(x = "year", y = "Tmax (°C)") +
  geom_smooth(method = lm)

lm_summertmax <- lm(meantmax ~ year, winnesummer)
summary(lm_summertmax)

```

### Winter Temperatures - Winneshiek County

```{r winter temps, message=F}

winnewinter <- tmaxdf %>%
  filter(countyfp==191 & (doy <= 59 | doy >= 335) & !is.na(tmax)) %>%
  group_by(year) %>%
  summarize(meantmax = mean(tmax))

ggplot(winnewinter, mapping = aes(x = year, y = meantmax)) +
  geom_point() +
  theme_bw() +
  labs(x = "year", y = "Tmax (°C)") +
  geom_smooth(method = lm)

lm_wintertmax <- lm(meantmax ~ year, winnewinter)
summary(lm_wintertmax)

```

### Multiple regression -- Quadratic time trend

```{r quadratic temp trend}

winnewinter$yearsq <- winnewinter$year^2

lm_wintertmaxquad <- lm(meantmax ~ year + yearsq, winnewinter)
summary(lm_wintertmaxquad)
winnewinter$fitted <- lm_wintertmaxquad$fitted.values

ggplot(winnewinter) +
  geom_point(mapping = aes(x = year, y = meantmax)) +
  geom_line(mapping = aes(x = year, y = fitted)) +
  theme_bw() +
  labs(x = "year", y = "tmax")

```

### Download NASS corn yield data

```{r yield download, results='hide'}

# set our API key with NASS
nassqs_auth(key = "DD0322AC-2B3C-34A0-B93C-52D5A8844518")

# parameters to query on 
params <- list(commodity_desc = "CORN", util_practice_desc = "GRAIN", prodn_practice_desc = "ALL PRODUCTION PRACTICES", year__GE = 1981, state_alpha = "IA")

# download
cornyieldsall <- nassqs_yields(params)

cornyieldsall$county_ansi <- as.numeric(cornyieldsall$county_ansi)
cornyieldsall$yield <- as.numeric(cornyieldsall$Value)

# clean and filter this dataset
cornyields <- select(cornyieldsall, county_ansi, county_name, yield, year) %>%
  filter(!is.na(county_ansi) & !is.na(yield))
cornyields <- tibble(cornyields)

```

## Assignment

### Question 1a: Extract Winneshiek County corn yields, fit a linear time trend, make a plot. Is there a significant time trend?

```{r, message=FALSE}
# county data
cornWinneshiek<-cornyields%>%
  filter(county_name == "WINNESHIEK")

# linear model
lm_Winneshiek <- lm(yield ~ year, cornWinneshiek)
summary(lm_Winneshiek)

# plot
ggplot(cornWinneshiek, mapping = aes(x = year, y = yield)) +
  geom_point() +
  geom_smooth(method = lm) +
  labs(x = "Year", y = "Yield")

```

\hstart

* There is a significant increase in yield over time.

\hstop

### Question 1b: Fit a quadratic time trend (i.e., year + year^2) and make a plot. Is there evidence for slowing yield growth? 

```{r}
#year^2
cornWinneshiek$yearsq <- cornWinneshiek$year^2

# linear model
lm_Winneshiek <- lm(yield ~ year + yearsq, cornWinneshiek)
summary(lm_Winneshiek)
cornWinneshiek$fitted <- lm_Winneshiek$fitted.values


# plot
ggplot(cornWinneshiek) +
  geom_point(mapping = aes(x = year, y = yield)) +
  geom_line(mapping = aes(x = year, y = fitted)) +
  theme_bw() +
  labs(x = "Year", y = "Yield")
```

\hstart

* The quadratic time trend does not show evidence of slowing yield growth.

\hstop

### Question 2 -- Time Series: Let's analyze the relationship between temperature and yields for the Winneshiek County time series. Use data on yield and summer avg Tmax. Is adding year or Tmax^2 to your model helpful? Make a plot and interpret the results.

```{r}
# join data
WinneshiekJoin<-winnesummer%>%
  inner_join(cornWinneshiek, by = "year")


# linear model
WinneshiekJoin$tempsq <-WinneshiekJoin$meantmax^2
lm_Winne <- lm(yield ~ meantmax + tempsq, WinneshiekJoin)
summary(lm_Winne)


# plot
WinneshiekJoin$quadfitted<-lm_Winne$fitted.values
ggplot(WinneshiekJoin) +
  geom_point(mapping = aes(x = meantmax, y = yield)) +
  geom_line(mapping = aes(x = meantmax, y = quadfitted)) +
  theme_bw() +
  labs(x = "Temperature", y = "Yield")

```

\hstart

* It is not helpful to add Tmax^2 to the model. 

\hstop

### Question 3 -- Cross-Section: Analyze the relationship between temperature and yield across all counties in 2018. Is there a relationship? Interpret the results.

```{r}
# get 2018 data
cyield18 <- cornyields %>% 
  filter(year == 2018) %>% 
  group_by(county_name) %>% 
  unique() %>% 
  filter(!is.na(county_ansi))

temp18 <- tmaxdf%>%
  group_by(countyfp)%>%
  filter(year == 2018)%>%
  filter(!is.na(tmax))%>%
  filter(doy >= 152 & doy <= 243) %>%
  summarize(meantmax = mean(tmax)) %>%
  rename(county_ansi = "countyfp")

temp18$county_ansi <- as.numeric(as.character(temp18$county_ansi))

#Joining the two dfs together
county_yield18 <- left_join(cyield18,temp18, by='county_ansi')

# lm
county_yield18$tempsq <-county_yield18$meantmax^2
lm_county18 <- lm(yield ~ meantmax + tempsq, county_yield18)
summary(lm_county18)

county_yield18$quadfitted<-lm_county18$fitted.values

#plot
ggplot(county_yield18, aes(x = meantmax, y = yield))+
  geom_point()+
  theme_bw()+
  labs(x = "Temperature", y = "Yield")

#plot quadratic
ggplot(county_yield18) +
  geom_point(mapping = aes(x = meantmax, y = yield)) +
  geom_line(mapping = aes(x = meantmax, y = quadfitted)) +
  theme_bw() +
  labs(x = "Temperature", y = "Yield")

```

\hstart

* There is not a trend but the quadratic, however, does show a trend. Looking at the quadratic, there is a decrease in yield for both cooler and hotter temperatures, the peak yield occurs closer to 28°C.

\hstop

### Question 4 -- Panel: One way to leverage multiple time series is to group all data into what is called a "panel" regression. Convert the county ID code ("countyfp" or "county_ansi") into factor using as.factor, then include this variable in a regression using all counties' yield and summer temperature data. How does the significance of your temperature coefficients (Tmax, Tmax^2) change? Make a plot comparing actual and fitted yields and interpret the results of your model.

```{r, message=FALSE}
# combine data
ttocombine <- tmaxdf%>%
  filter(doy >= 152 & doy <= 243) %>%
  group_by(countyfp, year)%>%
  rename(county_ansi = countyfp)%>%
  summarize(meantmax = mean(tmax))

#factor
ttocombine$county_ansi <- as.numeric(as.character(ttocombine$county_ansi))

#Joining the data
yieldtemp <- inner_join(cornyields,ttocombine,)%>%
  unique()

#square temp
yieldtemp$meantmaxsq <- yieldtemp$meantmax^2

#lm
lm_yieldtemp<-lm(yield ~county_ansi + meantmax + meantmaxsq + year, yieldtemp)
summary(lm_yieldtemp)

#fitted values
yieldtemp$fittedyield<-lm_yieldtemp$fitted.values

#plot
ggplot(yieldtemp, aes(x = fittedyield, y = yield))+
  geom_point()+
  geom_abline(color = "purple", size = 1)+
  labs(x = "Fitted Yield Values", y = "Yield")

```

\hstart

* Because the fitted values generally follow the actual yield values and there is a high p-value there is a significant goodness of fit for the model.

\hstop

### Question 5 -- Soybeans: Download NASS data on soybean yields and explore either a time series relationship for a given county, the cross-sectional relationship for a given year, or a panel across all counties and years.

```{r, results='hide', message=FALSE}

# set our API key with NASS
nassqs_auth(key = "DD0322AC-2B3C-34A0-B93C-52D5A8844518")

# parameters to query on 
params2 <- list(commodity_desc = "SOYBEANS", statisticcat_desc = "YIELD", prodn_practice_desc = "ALL PRODUCTION PRACTICES", year__GE = 1981, state_alpha = "IA")

# download
soyyieldsall <- nassqs_yields(params)

soyyieldsall$county_ansi <- as.numeric(soyyieldsall$county_ansi)
soyyieldsall$yield <- as.numeric(soyyieldsall$Value)

# clean and filter this dataset
soyyields <- select(soyyieldsall, county_ansi, county_name, yield, year) %>%
  filter(!is.na(county_ansi) & !is.na(yield))
soyyields <- tibble(soyyields)

# county data
soyWinneshiek<-soyyields%>%
  filter(county_name == "WINNESHIEK")

# linear model
lm_soy <- lm(yield ~ year, soyWinneshiek)
summary(lm_soy)

# plot
ggplot(soyWinneshiek, mapping = aes(x = year, y = yield)) +
  geom_point() +
  geom_smooth(method = lm) +
  labs(x = "Year", y = "Yield")

```


<!--chapter:end:07-weather_corn_regression.Rmd-->

